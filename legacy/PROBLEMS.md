# üêõ Academic Agent - Problem-Tracking & Fix-Prompts

> **Format**: Jedes Problem enth√§lt eine Beschreibung, Status und einen Fix-Prompt
>
> **WICHTIG**: Die Status-√úbersicht befindet sich immer **ganz unten** in diesem Dokument und muss bei √Ñnderungen aktualisiert werden!

---

## üìã TODO-Liste

### 1. Setup-Agent bleibt stehen ‚ùå

**Status**: Offen
**Priorit√§t**: Hoch
**Betroffene Komponente**: `setup-agent`, `/academicagent` Skill

**Problem-Beschreibung**:
Der Setup-Agent startet erfolgreich, bleibt dann aber stehen ohne weitere Interaktion. Der Agent spawnt, zeigt "Done (2 tool uses ¬∑ 11.4k tokens ¬∑ 1m 29s)", aber wartet dann auf User-Input ohne dass klar ist, wo/wie dieser gegeben werden soll.

**Symptome**:
```
‚è∫ setup-agent(Interaktives Recherche-Setup)
  ‚éø  Done (2 tool uses ¬∑ 11.4k tokens ¬∑ 1m 29s)

‚è∫ Der Setup-Agent hat erfolgreich gestartet und f√ºhrt dich nun durch die interaktive Konfiguration.
  N√§chster Schritt: Beantworte die Frage des Setup-Agents zu deiner konkreten Forschungsfrage...
```

**Root Cause**:
- Setup-Agent sollte eigentlich AskUserQuestion verwenden, um Fragen zu stellen
- Stattdessen "Done" ohne sichtbare Fragen
- Kommunikation zwischen Main Thread und Sub-Agent funktioniert nicht richtig

**Fix-Prompt**:
```
Problem: Der setup-agent im /academicagent Skill bleibt nach dem Spawn stehen und zeigt nur "Done", ohne die interaktiven Fragen zu stellen.

Aufgabe:
1. √ñffne agents/setup_agent.py
2. Analysiere, warum der Agent keine AskUserQuestion-Calls macht oder diese nicht sichtbar sind
3. Stelle sicher, dass der Agent die Fragen korrekt an den User weiterleitet
4. Teste, ob die Fragen im Main Thread ankommen und angezeigt werden

M√∂gliche Ursachen:
- Agent verwendet print() statt AskUserQuestion
- Agent-Output wird nicht korrekt zur√ºckgegeben
- Der Agent terminiert zu fr√ºh

Bitte behebe das Problem so, dass die interaktiven Fragen beim User ankommen.
```

---

### 2. TUI startet nicht automatisch / funktioniert nicht in Claude Code ‚ùå

**Status**: Offen
**Priorit√§t**: Mittel
**Betroffene Komponente**: `scripts/academicagent_wrapper.sh`, TUI-Modus

**Problem-Beschreibung**:
Der TUI-Modus (Terminal User Interface) kann nicht in Claude Code ausgef√ºhrt werden. Der Wrapper erkennt questionary als nicht installiert (obwohl Installation erfolgreich), und TUI-basierte interaktive Eingaben funktionieren grunds√§tzlich nicht in Claude Code's eingebettetem Terminal.

**Symptome**:
```bash
‚ùå TUI-Modus kann nicht in Claude Code ausgef√ºhr werden
Error: Exit code 1

\033[1;33m‚ö†Ô∏è   questionary nicht installiert\033[0m
```

**Root Cause**:
1. TUI ben√∂tigt echtes TTY (Terminal)
2. Claude Code's embedded terminal unterst√ºtzt keine interaktiven TUI-Prompts (questionary/prompt_toolkit)
3. Wrapper-Script versucht trotzdem, TUI zu starten

**Fix-Prompt**:
```
Problem: Der TUI-Modus im academicagent_wrapper.sh kann nicht in Claude Code gestartet werden und gibt irref√ºhrende Fehlermeldungen.

Aufgabe:
1. √ñffne scripts/academicagent_wrapper.sh
2. F√ºge eine Erkennung hinzu, ob das Script in einem echten Terminal (TTY) oder in Claude Code l√§uft
3. Wenn kein TTY verf√ºgbar ist:
   - Zeige klare Warnung: "TUI-Modus erfordert echtes Terminal. Bitte au√üerhalb von Claude Code ausf√ºhren."
   - Biete Fallback an: Chat-basiertes Setup verwenden
4. Verhindere den Versuch, questionary zu installieren, wenn ohnehin kein TTY verf√ºgbar ist

Test:
- F√ºhre Script in Claude Code aus ‚Üí sollte klare Warnung zeigen
- F√ºhre Script in echtem Terminal aus ‚Üí sollte TUI korrekt starten

Bitte implementiere die TTY-Erkennung und sinnvolle Fallbacks.
```

---

### 3. Optionen werden nicht/unvollst√§ndig angezeigt im Chat ‚ùå

**Status**: Offen
**Priorit√§t**: Hoch
**Betroffene Komponente**: `setup-agent`, AskUserQuestion-Integration

**Problem-Beschreibung**:
Wenn der Setup-Agent im Chat-Modus l√§uft, werden die Auswahloptionen f√ºr Forschungsfrage, Recherche-Modus etc. nicht oder nur unvollst√§ndig angezeigt. Der User sieht nur Text wie "Welche Forschungsfrage m√∂chtest du verwenden?" aber keine konkreten Optionen zur Auswahl.

**Symptome**:
```
‚è∫ Der Setup-Agent wartet jetzt auf deine Antwort.
  Welche Forschungsfrage m√∂chtest du verwenden?
  Du kannst einfach die Nummer (1, 2 oder 3) schreiben...

  [Aber Optionen 1, 2, 3 werden nie gezeigt]
```

**Root Cause**:
- Setup-Agent gibt Optionen zur√ºck, aber diese werden nicht an AskUserQuestion weitergeleitet
- Oder: Agent-Output wird nicht korrekt geparsed im Main Thread
- AskUserQuestion-Tool wird vom Sub-Agent nicht richtig verwendet

**Fix-Prompt**:
```
Problem: Der setup-agent zeigt im Chat-Modus die Auswahloptionen nicht an. User sieht Fragen, aber keine konkreten Antwortm√∂glichkeiten.

Aufgabe:
1. √ñffne agents/setup_agent.py
2. Stelle sicher, dass der Agent AskUserQuestion korrekt verwendet mit options-Array
3. Pr√ºfe, ob die Agent-Response korrekt formatiert ist
4. Teste, ob die Optionen im Main Thread ankommen und als Auswahl angezeigt werden

Beispiel f√ºr korrektes AskUserQuestion:
{
  "questions": [{
    "question": "Welche Forschungsfrage m√∂chtest du verwenden?",
    "header": "Frage",
    "options": [
      {"label": "Option 1: ...", "description": "Beschreibung"},
      {"label": "Option 2: ...", "description": "Beschreibung"}
    ],
    "multiSelect": false
  }]
}

Bitte stelle sicher, dass der setup-agent AskUserQuestion mit vollst√§ndigen options verwendet, nicht nur print().
```

---

### 4. Agent-Verschachtelungsproblem (zu viele Ebenen) ‚ùå

**Status**: Offen
**Priorit√§t**: Kritisch
**Betroffene Komponente**: `/academicagent` Skill, `orchestrator-agent`, Agent-Architektur

**Problem-Beschreibung**:
Die aktuelle Architektur spawnt zu viele verschachtelte Agent-Ebenen:
- Ebene 1: Claude Code
- Ebene 2: /academicagent Skill
- Ebene 3: setup-agent (via Task tool)
- Ebene 4: orchestrator-agent (via Task tool)
- Ebene 5: ‚ùå browser-agent, scoring-agent, extraction-agent (k√∂nnen nicht mehr gespawnt werden)

**Fehlermeldung**:
```
Error: nested sessions are not supported
Orchestrator-Agent kann keine weiteren Sub-Agents spawnen
```

**Root Cause**:
- Claude Code hat Limits f√ºr Agent-Verschachtelungstiefe
- Orchestrator sitzt zu tief in der Hierarchie (Ebene 4)
- Kann daher keine weiteren Task()-Calls f√ºr browser/scoring/extraction machen

**Fix-Prompt**:
```
Problem: Die Agent-Hierarchie im /academicagent Skill ist zu tief verschachtelt. Der orchestrator-agent kann keine Sub-Agents (browser, scoring, extraction) mehr spawnen.

Aufgabe - Architektur-Refactoring:
1. √ñffne skills/academicagent/academicagent.prompt.md
2. √Ñndere die Architektur wie folgt:

VORHER (funktioniert nicht):
/academicagent ‚Üí setup-agent (Task) ‚Üí orchestrator-agent (Task) ‚Üí ‚ùå weitere Agents

NACHHER (soll funktionieren):
/academicagent ‚Üí orchestrator-agent (Task direkt) ‚Üí browser/scoring/extraction (Task)

3. Konkrete √Ñnderungen:
   - Entferne setup-agent als separaten Task-Spawn
   - Integriere Setup-Logik direkt in /academicagent Main Thread (mit AskUserQuestion)
   - Spawne orchestrator-agent direkt von /academicagent (Ebene 2 statt Ebene 4)
   - Orchestrator kann dann problemlos seine Sub-Agents spawnen (Ebene 3)

4. Setup-Flow vereinfachen:
   - /academicagent fragt User-Fragen direkt mit AskUserQuestion
   - Generiert run_config.json selbst
   - Spawnt dann orchestrator mit fertiger Config

Bitte refactore die Architektur, um eine Verschachtelungsebene einzusparen.
```

---

### 5. orchestrator-agent h√§ngt bei "Verarbeite Inputs..." ‚è≥

**Status**: Offen
**Priorit√§t**: Hoch
**Betroffene Komponente**: `orchestrator-agent`

**Problem-Beschreibung**:
Der orchestrator-agent spawnt erfolgreich, zeigt dann aber nur "Verarbeite Inputs..." und terminiert nicht bzw. l√§uft endlos.

**Symptome**:
```
‚è∫ orchestrator-agent(Starte 7-Phasen Workflow)
  ‚éø  Running (45s elapsed)
  Verarbeite Inputs...
  [Agent h√§ngt hier]
```

**Root Cause**:
- Vermutlich Endlos-Schleife oder blockierender Call
- Agent wartet auf Input, der nie kommt
- Oder: Verschachtelungsproblem verhindert weitere Tool-Calls

**Fix-Prompt**:
```
Problem: Der orchestrator-agent startet, zeigt "Verarbeite Inputs..." und h√§ngt dann ohne weiteren Output.

Aufgabe:
1. √ñffne agents/orchestrator_agent.py
2. Analysiere die Startup-Logik:
   - Was macht der Agent beim "Verarbeite Inputs..."?
   - Gibt es blockierende Calls?
   - Wartet er auf externen Input?
3. F√ºge Debug-Logging hinzu, um zu sehen wo der Agent h√§ngt
4. Stelle sicher, dass der Agent entweder:
   - Erfolgreich die Sub-Agents spawnt
   - Oder: Fehler klar zur√ºckgibt

Teste mit:
- Vereinfachter Test-Config (nur 1 Quelle, Quick-Modus)
- Pr√ºfe, ob Agent bis zum browser-agent-Spawn kommt

Bitte behebe das H√§ngen-Problem im Orchestrator.
```

---

### 6. Live Monitor - Automatisches Chrome-Fenster statt Terminal-Befehl ‚ùå

**Status**: Offen
**Priorit√§t**: Mittel
**Betroffene Komponente**: Live Monitor, `scripts/live_monitor.py`, Orchestrator

**Problem-Beschreibung**:
Der aktuelle Live Monitor erfordert, dass der User einen Terminal-Befehl kopiert und manuell in einem neuen Terminal-Fenster ausf√ºhrt. Dies ist umst√§ndlich und nicht benutzerfreundlich.

**Aktuelles Verhalten**:
```
‚è∫ Live Monitor verf√ºgbar
  Kopiere folgenden Befehl und f√ºhre ihn in einem neuen Terminal aus:
  python scripts/live_monitor.py --run-id=run_xyz
```

**Gew√ºnschtes Verhalten**:
- Chrome-Fenster √∂ffnet sich automatisch
- Live Monitor wird direkt im Browser angezeigt
- Keine manuelle Kopier-/Terminal-Aktion n√∂tig
- Automatische Integration in den Workflow

**Root Cause**:
- Live Monitor l√§uft als separater Python-Prozess
- Keine automatische Browser-Integration
- User muss manuell Terminal-Befehle ausf√ºhren

**Fix-Prompt**:
```
Problem: Der Live Monitor erfordert manuelles Kopieren und Ausf√ºhren eines Terminal-Befehls. Dies soll automatisiert werden - stattdessen soll sich ein Chrome-Fenster automatisch √∂ffnen.

Aufgabe - Live Monitor Automatisierung:

1. Analysiere die aktuelle Live-Monitor-Implementierung:
   - √ñffne scripts/live_monitor.py
   - Verstehe, wie der Monitor aktuell gestartet wird
   - Pr√ºfe, wo im Orchestrator der Monitor-Befehl ausgegeben wird

2. Implementiere automatisches Browser-Opening:
   - Live Monitor soll automatisch im Hintergrund starten
   - Verwende Python's webbrowser.open() oder subprocess f√ºr Chrome
   - Chrome-Fenster soll sich automatisch mit der Monitor-URL √∂ffnen

3. Entferne die manuelle Kopier-Anweisung:
   - L√∂sche Output wie "Kopiere folgenden Befehl..."
   - Zeige stattdessen: "‚úÖ Live Monitor ge√∂ffnet in Chrome"

4. Browser-Integration:
   - Live Monitor soll einen lokalen HTTP-Server starten (z.B. Port 8000)
   - Server l√§uft im Hintergrund (subprocess/daemon)
   - Chrome √∂ffnet automatisch `http://localhost:8000?run_id=xyz`
   - Optional: CDP-Chrome verwenden (bereits l√§uft auf Port 9222)

5. Cleanup-Handling:
   - Server soll beim Workflow-Ende automatisch stoppen
   - Oder: Server l√§uft weiter, aber nur f√ºr aktiven Run

Technische Ans√§tze:
Option A: Separater HTTP-Server (Flask/SimpleHTTPServer)
Option B: CDP-Chrome Tab √∂ffnen und HTML rendern
Option C: Datei-basiert (HTML schreiben + file:// URL √∂ffnen)

Bitte implementiere die automatische Browser-Integration f√ºr den Live Monitor.
```

---

### 7. Agent & Skill Prompt-Qualit√§t Validierung ‚úÖ

**Status**: ‚úÖ Gel√∂st (2026-02-23)
**Priorit√§t**: Mittel
**Betroffene Komponente**: Alle Agents & Skills (setup, orchestrator, browser, scoring, extraction, /academicagent)

**Problem-Beschreibung**:
Es existiert keine systematische Bewertung der Prompt-Qualit√§t f√ºr alle Agents und Skills. Unklar ist, welche Prompts gut strukturiert sind und welche umstrukturiert werden sollten.

**Ziel**:
Eine Bewertungsmatrix (Skala 0-10) f√ºr alle Agents und Skills erstellen:
- **0** = Perfekt, keine √Ñnderung n√∂tig
- **10** = Sofort umstrukturieren/√§ndern
- **5** = Mittlerer Handlungsbedarf

Bewertung soll umfassen:
- setup-agent.py
- orchestrator-agent.py
- browser-agent.py
- scoring-agent.py
- extraction-agent.py
- /academicagent Skill
- academicagent_wrapper.sh

F√ºr jeden Score: Begr√ºndung, warum dieser Wert vergeben wurde.

**Fix-Prompt**:
```
Problem: Keine systematische Bewertung der Prompt-Qualit√§t f√ºr Agents und Skills vorhanden.

Aufgabe - Prompt-Qualit√§t Validierung & Bewertung:

1. Erstelle eine Bewertungsmatrix (0-10 Skala) f√ºr folgende Komponenten:
   - setup-agent
   - orchestrator-agent
   - browser-agent
   - scoring-agent
   - extraction-agent
   - /academicagent Skill (skills/academicagent/academicagent.prompt.md)
   - academicagent_wrapper.sh

2. Bewertungsskala:
   - 0 = Perfekt, keine √Ñnderung n√∂tig
   - 10 = Sofort umstrukturieren/√§ndern
   - 5 = Mittlerer Handlungsbedarf

3. F√ºr jeden Score bitte angeben:
   - Status-Emoji (üü¢ = 0-3, üü° = 4-5, üü† = 6-7, üî¥ = 8-10)
   - Begr√ºndung (Was ist gut? Was ist schlecht?)
   - Konkrete √Ñnderungsempfehlung (falls Score > 2)

4. Erstelle eine Tabellen-Matrix mit:
   - Komponente | Score | Status | Begr√ºndung | √Ñnderungsempfehlung

5. F√ºge eine Ziel-Analyse hinzu:
   - Wie w√ºrde der Score nach Refactoring aussehen?
   - Was ist der Gesamt-Impact (Durchschnitt vorher/nachher)?

Bitte analysiere alle Prompt-Dateien und erstelle die vollst√§ndige Bewertungsmatrix.
```

**L√∂sung (2026-02-23)**:
‚úÖ Vollst√§ndige Bewertungsmatrix erstellt in: `PROMPT_QUALITY_ASSESSMENT.md`
- Alle 7 Komponenten systematisch validiert
- Scores von 1-7/10 vergeben (Durchschnitt: 3.3/10)
- Detaillierte Analysen mit Refactoring-Empfehlungen
- Validierungs-Report best√§tigt Assessments
- Hauptprobleme: orchestrator-agent (7/10), browser-agent (4/10), academicagent Skill (5/10)

**Ergebnis**:
- scoring-agent.md: Beste Datei (1/10) üü¢
- Refactoring-Aufwand: 3-4 Wochen @ 20h/Woche
- Quick Win: orchestrator-agent Refactoring = 50% Impact

---

## üìä Status-√úbersicht

| Problem | Status | Priorit√§t | Komponente |
|---------|--------|-----------|------------|
| #1: Setup-Agent bleibt stehen | ‚ùå Offen | Hoch | setup-agent |
| #2: TUI startet nicht in Claude Code | ‚ùå Offen | Mittel | wrapper script |
| #3: Optionen nicht angezeigt | ‚ùå Offen | Hoch | setup-agent |
| #4: Agent-Verschachtelung | ‚ùå Offen | **Kritisch** | Architektur |
| #5: Orchestrator h√§ngt | ‚ùå Offen | Hoch | orchestrator-agent |
| #6: Live Monitor Auto-Chrome | ‚ùå Offen | Mittel | live_monitor.py |
| #7: Prompt-Qualit√§t Validierung | ‚úÖ Gel√∂st | Mittel | Alle Agents/Skills |

**Gesamt**: 7 Probleme | 1 Gel√∂st | 6 Offen

---

## üéØ Empfohlene Fix-Reihenfolge

1. **Kritisch - Problem #4**: Agent-Verschachtelung beheben (Architektur-Refactoring)
   - Blockt alle anderen Features
   - Orchestrator muss Sub-Agents spawnen k√∂nnen

2. **Hoch - Problem #1**: Setup-Agent Interaction fixen
   - Nach Architektur-Fix: Setup in Main Thread integrieren
   - AskUserQuestion korrekt verwenden

3. **Hoch - Problem #3**: Optionen-Anzeige fixen
   - Sollte durch #1 gel√∂st werden
   - Falls nicht: Separater Fix n√∂tig

4. **Hoch - Problem #5**: Orchestrator-H√§ngen beheben
   - Nach #4: Sollte durch reduzierte Verschachtelung gel√∂st sein
   - Falls nicht: Debug-Logging hinzuf√ºgen

5. **Mittel - Problem #6**: Live Monitor Auto-Chrome
   - UX-Verbesserung, nicht kritisch
   - Nach Core-Fixes implementieren

6. **Mittel - Problem #2**: TUI-TTY-Erkennung
   - Nice-to-have f√ºr bessere UX
   - Nicht kritisch f√ºr Core-Funktionalit√§t

7. **Mittel - Problem #7**: Prompt-Qualit√§t Validierung
   - Kann parallel zu anderen Fixes laufen
   - Hilft bei langfristiger Code-Qualit√§t

---

## ‚ûï Neue Probleme hinzuf√ºgen

**Format f√ºr neue Probleme**:

```markdown
### X. [Kurzer Titel] ‚ùå

**Status**: Offen / In Bearbeitung / Gel√∂st
**Priorit√§t**: Niedrig / Mittel / Hoch / Kritisch
**Betroffene Komponente**: [Datei/Agent/Script]

**Problem-Beschreibung**:
[Detaillierte Beschreibung des Problems]

**Symptome**:
```
[Fehlermeldungen, Logs, Output-Beispiele]
```

**Root Cause**:
[Vermutete Ursache / Analyse]

**Fix-Prompt**:
```
Problem: [Knappe Zusammenfassung]

Aufgabe:
1. [Schritt 1]
2. [Schritt 2]
...

Bitte behebe das Problem.
```
```

---

**Letzte Aktualisierung**: 2026-02-23 - Problem #7 gel√∂st (Prompt-Qualit√§t Validierung abgeschlossen)
