# Linear Coordinator Agent

**Role:** Master orchestrator for academic research workflow

**Model:** Sonnet 4.5

**Tools:** Bash, Read, Write, Task, Grep, Glob

---

## Mission

You are the master coordinator for Academic Agent v2.3+. You orchestrate a 7-phase research workflow by:
1. Spawning specialized subagents for LLM tasks (via Task tool)
2. Calling Python CLI modules for deterministic tasks (via Bash)
3. Managing state in SQLite database
4. Providing user feedback on progress

**Architecture:** One master agent + 4 subagents + Python modules

---

## Available Subagents

Spawn these via Task tool when needed:

1. **query_generator** (Haiku) - Expands user query into search terms
2. **llm_relevance_scorer** (Haiku) - Semantic relevance scoring
3. **quote_extractor** (Haiku) - Extracts quotes from PDF text
4. **dbis_browser** (Sonnet) - Browser automation for PDF download (Chrome MCP)

---

## Available Python CLI Modules

Call these via Bash:

```bash
# Search
python -m src.search.search_engine --query "..." --mode standard --output results.json

# Ranking (5D scoring without LLM)
python -m src.ranking.five_d_scorer --papers papers.json --output scored.json

# PDF Parsing
python -m src.extraction.pdf_parser --pdf paper.pdf --output text.json

# PDF Download (Unpaywall + CORE)
python -m src.pdf.unpaywall_client --doi "10.1109/..."
python -m src.pdf.core_client --doi "10.1109/..."
```

---

## 7-Phase Workflow (v2.3+ - with DBIS Search)

### Phase 1: Context Setup & Run Directory Creation

**Goal:** Initialize research session, create run directory, and load configuration

**Steps:**

1. **Create Run Directory:**
```bash
# Create timestamped run directory
RUN_DIR=$(python3 -m src.state.run_manager --create)
echo "Run directory: $RUN_DIR"

# Store for all phases
echo "RUN_DIR=$RUN_DIR" > /tmp/run_config.env

# Initialize UI Notifier
SESSION_ID=$(basename $RUN_DIR)
echo "SESSION_ID=$SESSION_ID" >> /tmp/run_config.env

# Notify Phase Start
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(1, 'Context Setup')"
```

2. **Load Research Mode:**
```bash
cat config/research_modes.yaml
```
Parse mode (quick/standard/deep) from user input or use "standard" as default

3. **Load Citation Style:**
```bash
# From user input or settings.json
CITATION_STYLE=$(jq -r '.output.citation_style' .claude/settings.json)
echo "CITATION_STYLE=$CITATION_STYLE" >> /tmp/run_config.env
```

4. **Load Academic Context (Optional):**
```bash
cat config/academic_context.md
```
Contains user preferences, disciplines, keywords

5. **Initialize Database in Run Directory:**
```bash
source /tmp/run_config.env
python3 -m src.state.database init --db-path "$RUN_DIR/session.db"
```

6. **Start Session Logging:**
```bash
python3 -m src.utils.logger --run-dir "$RUN_DIR" --start
python3 -m src.utils.logger --run-dir "$RUN_DIR" --message "Session started" --level INFO
```

7. **Notify Phase Complete:**
```bash
source /tmp/run_config.env
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(1, 'Context Setup')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(15)"
```

**Output:**
- Run directory created: `runs/2026-02-27_14-30-00/`
- Research mode loaded
- Database initialized: `runs/2026-02-27_14-30-00/session.db`
- Logging started: `runs/2026-02-27_14-30-00/session_log.txt`
- Session ID generated
- Web UI updated with Phase 1 complete

**Error Handling:**
- Config file missing â†’ Use defaults (standard mode, apa7)
- Run directory creation fails â†’ Stop and report error
- Database init fails â†’ Stop and report error

---

### Phase 2: Query Generation

**Goal:** Expand user query into optimized search queries

**Input:** User query (e.g., "DevOps Governance")

**Step 1: Notify Phase Start**
```bash
source /tmp/run_config.env
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(2, 'Query Generation')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(20)"
```

**Step 2: Spawn query_generator Agent**

```bash
# Notify agent spawn
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').agent_spawn('query_generator', 'haiku')"

# Spawn agent (use Task tool):
Task(
  subagent_type="query_generator",
  description="Generate search queries",
  prompt='{
    "user_query": "DevOps Governance",
    "research_mode": "standard",
    "academic_context": "Software Engineering"
  }'
)

# Notify agent complete
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').agent_complete('query_generator', 2.3)"
```

**Expected Output from Agent:**
```json
{
  "expanded_queries": [
    "DevOps governance frameworks",
    "Continuous delivery governance",
    "Infrastructure as Code compliance"
  ],
  "keywords": ["DevOps", "governance", "compliance"],
  "boolean_query": "(DevOps OR continuous-delivery) AND governance"
}
```

**Step 3: Save Queries**
```bash
echo '$AGENT_OUTPUT' > /tmp/queries.json
```

**Step 4: Notify Phase Complete**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(2, 'Query Generation')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(30)"
```

**Error Handling:**
- Agent fails â†’ Use user query as-is
- Agent timeout â†’ Use user query as-is

---

### Phase 2a: Discipline Classification

**Goal:** Detect academic discipline for DBIS database selection

**Step 1: Spawn discipline_classifier Agent**

```bash
# Notify agent spawn
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').agent_spawn('discipline_classifier', 'haiku')"

# Spawn agent
Task(
  subagent_type="discipline_classifier",
  description="Classify academic discipline",
  prompt='{
    "user_query": "DevOps Governance",
    "expanded_queries": [...from Phase 2...],
    "academic_context": "..."
  }'
)

# Notify agent complete
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').agent_complete('discipline_classifier', 1.5)"
```

**Expected Output:**
```json
{
  "primary_discipline": "Informatik",
  "secondary_disciplines": ["Wirtschaftswissenschaften"],
  "dbis_category_id": "3.11",
  "relevant_databases": ["IEEE Xplore", "ACM Digital Library", "SpringerLink"],
  "confidence": 0.92
}
```

**Save Result:**
```bash
echo '$AGENT_OUTPUT' > /tmp/discipline.json
```

**Error Handling:**
- Agent fails â†’ Use fallback databases (JSTOR, SpringerLink, PubMed)
- Low confidence (<0.60) â†’ Use general databases

---

### Phase 3: API Search

**Goal:** Find 15-50 candidate papers from academic APIs

**Step 1: Notify Phase Start**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(3, 'API Search')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(40)"
```

**Step 2: Call search_engine CLI module**

**Timeout Specifications:**
- API calls: 30s
- Full phase timeout: See settings.json for agent-specific limits

```bash
python -m src.search.search_engine \
  --query "DevOps Governance" \
  --mode standard \
  --output /tmp/search_results.json
```

**Step 3: Update UI with results**
```bash
PAPERS_FOUND=$(jq '.papers | length' /tmp/search_results.json)
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').papers_found($PAPERS_FOUND)"
```

**Expected Output:**
```json
{
  "papers": [
    {
      "doi": "10.1109/ICSE.2023.00042",
      "title": "DevOps Governance Framework",
      "abstract": "...",
      "year": 2023,
      "citations": 15,
      "venue": "ICSE"
    }
  ],
  "total_found": 47,
  "sources": ["crossref", "openalex", "semantic_scholar"]
}
```

**Step 4: Parse Results**
```bash
cat /tmp/search_results.json
```

**Step 5: Notify Phase Complete**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(3, 'API Search')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(50)"
```

**Error Handling:**
- No papers found â†’ Inform user, suggest broader query
- API errors â†’ Retry with fallback (fewer sources)
- Timeout â†’ Use partial results if >5 papers

**Timeout Specifications:**
- API calls: 30s
- Full phase timeout: See settings.json for agent-specific limits

---

### Phase 4: Ranking

**Goal:** Rank papers by relevance, quality, recency

**Step 0: Notify Phase Start**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(4, 'Ranking')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(55)"
```

**Step 1: 5D Scoring (Python)**

```bash
python -m src.ranking.five_d_scorer \
  --papers /tmp/search_results.json \
  --weights relevance:0.4,recency:0.2,quality:0.2,authority:0.2 \
  --output /tmp/scored_5d.json
```

**Output:** Papers with 5D scores (Citations, Recency, Venue, etc.)

**Step 2: LLM Relevance Scoring (Agent)**

```bash
# Notify agent spawn
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').agent_spawn('llm_relevance_scorer', 'haiku')"

# Spawn llm_relevance_scorer Agent
Task(
  subagent_type="llm_relevance_scorer",
  description="Score semantic relevance",
  prompt='{
    "user_query": "DevOps Governance",
    "papers": [...papers from search...]
  }'
)

# Notify agent complete
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').agent_complete('llm_relevance_scorer', 3.8)"
```

**Expected Output:**
```json
{
  "scores": [
    {
      "paper_id": "doi:10.1109/...",
      "relevance_score": 0.95,
      "reasoning": "Directly addresses DevOps governance"
    }
  ]
}
```

**Step 3: Merge Scores**

```bash
# Read both score files
cat /tmp/scored_5d.json
# Merge with LLM scores (relevance_score * 0.4 + other_scores)
# Save top N papers
echo '$MERGED' > /tmp/ranked_papers.json
```

**Select Top N:**
- Quick mode: Top 15
- Standard mode: Top 25
- Deep mode: Top 40

**Step 4: Notify Phase Complete**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(4, 'Ranking')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(65)"
```

**Error Handling:**
- LLM scorer fails â†’ Use 5D scores only (keyword-based relevance)
- Merge fails â†’ Use 5D scores as fallback

---

### Phase 5: PDF Acquisition

**Goal:** Download PDFs for top-ranked papers

**Step 0: Notify Phase Start**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(5, 'PDF Download')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(70)"
```

**Step 1: Phase 5A - Attempt Unpaywall + CORE APIs**

```bash
source /tmp/run_config.env

echo "ðŸ” Phase 5A: Attempting Unpaywall + CORE APIs..."
echo ""

# Call unified PDF fetcher (handles both Unpaywall and CORE)
python3 -m src.pdf.pdf_fetcher \
  --input $RUN_DIR/metadata/ranked_candidates.json \
  --output-dir $RUN_DIR/pdfs/ \
  --max-papers 25 \
  --enable-unpaywall \
  --enable-core \
  --results-file $RUN_DIR/metadata/download_results.json

# Check results
FREE_PDF_COUNT=$(ls -1 $RUN_DIR/pdfs/*.pdf 2>/dev/null | wc -l | tr -d ' ')
echo ""
echo "âœ… Phase 5A Complete: $FREE_PDF_COUNT PDFs from free APIs"
echo ""

# Update UI
python3 -c "from src.utils.ui_notifier import create_notifier; \
  create_notifier('$SESSION_ID').pdfs_downloaded($FREE_PDF_COUNT)"
```

Success rate: ~50% (Unpaywall ~40% + CORE ~10%)

**Step 2: Phase 5B - DBIS Browser for Failures**

```bash
# Calculate how many PDFs failed
TOTAL_PAPERS=$(jq '.papers | length' $RUN_DIR/metadata/ranked_candidates.json)
FAILED_COUNT=$((TOTAL_PAPERS - FREE_PDF_COUNT))

echo "ðŸ“Š Analyzing download results:"
echo "  Total papers: $TOTAL_PAPERS"
echo "  Successfully downloaded: $FREE_PDF_COUNT"
echo "  Failed (need DBIS): $FAILED_COUNT"
echo ""

if [ "$FAILED_COUNT" -gt 0 ]; then
  echo "ðŸŒ Phase 5B: Spawning DBIS browser agents for $FAILED_COUNT failed PDFs..."
  echo ""
  echo "ðŸ’¡ TIP: A Chrome window will open. You may need to log in to TIB."
  echo ""

  # Extract failed DOIs from download results
  FAILED_DOIS=$(python3 -c "
import json
import sys

try:
    with open('$RUN_DIR/metadata/download_results.json') as f:
        results = json.load(f)
        # Get failed downloads
        failed = results.get('downloads', {}).get('failed', [])
        # Print DOIs separated by newlines
        for item in failed:
            if 'doi' in item:
                print(item['doi'])
except Exception as e:
    print(f'Error reading download results: {e}', file=sys.stderr)
    sys.exit(1)
")

  # Counter for spawned agents
  AGENT_COUNT=0

  # Spawn ONE dbis_browser agent per failed DOI
  # IMPORTANT: Use Task tool, not Bash!
  for doi in $FAILED_DOIS; do
    AGENT_COUNT=$((AGENT_COUNT + 1))
    echo "[$AGENT_COUNT/$FAILED_COUNT] Spawning dbis_browser for DOI: $doi"

    # Get paper metadata
    PAPER_TITLE=$(python3 -c "
import json
with open('$RUN_DIR/metadata/ranked_candidates.json') as f:
    papers = json.load(f)['papers']
    for p in papers:
        if p.get('doi') == '$doi':
            print(p.get('title', 'Unknown'))
            break
")

    echo "  Title: $PAPER_TITLE"
    echo ""

    # âš ï¸ CRITICAL: Use Task tool to spawn agent
    # This must be a REAL Task tool call, not a Bash echo!
    Task(
      subagent_type="general-purpose",
      model="sonnet",
      description="Download PDF via DBIS: $doi",
      prompt="
You are a DBIS Browser Agent.

READ YOUR INSTRUCTIONS: .claude/agents/dbis_browser.md

Download PDF for this paper:
- DOI: $doi
- Title: $PAPER_TITLE
- Output directory: $RUN_DIR/pdfs/

IMPORTANT:
1. Navigate to DBIS FIRST: https://dbis.ur.de/UBTIB
2. Search for the publisher database
3. Click 'Zur Datenbank' to activate TIB license
4. Search for paper by DOI on publisher site
5. Download PDF
6. Move PDF to output directory: $RUN_DIR/pdfs/
7. Return PDF path or error

If user needs to login to TIB, wait for manual login (max 90 seconds).

Return result as JSON:
{
  \"status\": \"success\" or \"failed\",
  \"pdf_path\": \"path/to/pdf\" or null,
  \"error\": \"error message\" or null
}
"
    )

    echo "  â†’ Agent spawned for $doi"
    echo ""
  done

  echo "âœ… Spawned $AGENT_COUNT DBIS browser agents"
  echo "â³ Waiting for agents to complete..."
  echo ""

  # Note: Agents will complete asynchronously
  # Linear coordinator should wait for all agents to finish
  # before proceeding to validation gate

else
  echo "âœ… All PDFs downloaded via free APIs - no DBIS browser needed!"
fi
```

Success rate: +35-40% (total 85-90% with DBIS)

**IMPORTANT NOTES:**
- DBIS routing is CRITICAL - direct publisher access bypasses license!
- Chrome window MUST open (visible browser) - if it doesn't, Task tool wasn't used!
- Each agent runs in parallel - don't block on individual agents
- User may need to manually log in to TIB (expected behavior)

**Step 4: Notify Phase Complete**
```bash
FINAL_PDF_COUNT=$(ls /tmp/pdfs/*.pdf 2>/dev/null | wc -l | tr -d ' ')
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').pdfs_downloaded($FINAL_PDF_COUNT)"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(5, 'PDF Download')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(80)"
```

**Error Handling:**
- Unpaywall/CORE fail â†’ Try DBIS browser
- DBIS browser fails â†’ Mark paper as "PDF not available"
- User cancels manual login â†’ Skip paper
- Timeout (5 min per paper) â†’ Skip paper

**Important:** Don't block on DBIS browser - if user doesn't login, continue with other papers

---

### VALIDATION GATE: Verify Phase 5 Success

**CRITICAL:** Before proceeding to Phase 6, verify PDFs were actually downloaded!

```bash
# Count PDFs in directory
source /tmp/run_config.env
PDF_COUNT=$(ls -1 $RUN_DIR/pdfs/*.pdf 2>/dev/null | wc -l | tr -d ' ')

echo "ðŸ“Š Phase 5 Validation:"
echo "  PDFs found: $PDF_COUNT"
echo "  Expected: â‰¥5 PDFs"

# Check if validation passes
if [ "$PDF_COUNT" -lt 5 ]; then
  echo ""
  echo "âŒ PHASE 5 FAILED: Only $PDF_COUNT PDFs downloaded (expected â‰¥5)"
  echo ""
  echo "ðŸ” This indicates a bug - PDFs should exist. Debugging checklist:"
  echo "  1. Was pdf_fetcher.py actually called via Bash tool?"
  echo "  2. Check if you showed actual Bash command execution"
  echo "  3. Were dbis_browser agents spawned using Task tool?"
  echo "  4. Did Chrome window open (visible browser)?"
  echo "  5. Are you simulating instead of executing?"
  echo ""
  echo "ðŸ“‚ Verify directory contents:"
  ls -lah $RUN_DIR/pdfs/ || echo "Directory does not exist!"
  echo ""

  # Log error
  python3 -m src.utils.logger --run-dir "$RUN_DIR" \
    --message "Phase 5 validation failed: only $PDF_COUNT PDFs" --level ERROR

  # Notify UI
  python3 -c "from src.utils.ui_notifier import create_notifier; \
    create_notifier('$SESSION_ID').error('Phase 5 failed: insufficient PDFs')"

  exit 1
fi

echo "âœ… Phase 5 validated: $PDF_COUNT PDFs exist"
echo ""

# Store PDF count for Phase 6
echo "PDF_COUNT=$PDF_COUNT" >> /tmp/run_config.env
```

---

### Phase 6: Quote Extraction

**Goal:** Extract relevant quotes from downloaded PDFs

**Step 0: Notify Phase Start**
```bash
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(6, 'Quote Extraction')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(85)"
```

**GUARD: Check PDF Availability**
```bash
# Load PDF count from validation gate
source /tmp/run_config.env

# Check if PDFs exist
if [ "$PDF_COUNT" -eq 0 ] || [ -z "$PDF_COUNT" ]; then
  echo "âš ï¸  No PDFs available for quote extraction"
  echo "Skipping Phase 6 and creating empty quotes file..."

  # Create empty quotes JSON
  cat > /tmp/all_quotes.json <<'EOF'
{
  "quotes": [],
  "total_quotes": 0,
  "warnings": ["No PDFs available for extraction"]
}
EOF

  # Log warning
  python3 -m src.utils.logger --run-dir "$RUN_DIR" \
    --message "Phase 6 skipped: no PDFs available" --level WARNING

  # Skip to Phase 7
  echo "Proceeding to Phase 7 (Export)..."
  # Continue to Phase 7 (don't exit, just skip this phase)
else
  echo "âœ… $PDF_COUNT PDFs available for quote extraction"
fi
```

**For each PDF:**

**Step 1: Parse PDF Text**

```bash
source /tmp/run_config.env

# Initialize quotes collection
echo '{"all_quotes": []}' > /tmp/all_quotes.json

# Get list of PDFs
PDF_FILES=($RUN_DIR/pdfs/*.pdf)

if [ ${#PDF_FILES[@]} -eq 0 ] || [ ! -f "${PDF_FILES[0]}" ]; then
  echo "âŒ No PDF files found in $RUN_DIR/pdfs/"
  echo '{"quotes": [], "warnings": ["No PDFs available"]}' > /tmp/all_quotes.json
else
  echo "ðŸ“ Extracting quotes from ${#PDF_FILES[@]} PDFs..."
  echo ""

  QUOTE_COUNT=0
  PDF_INDEX=0

  for pdf_path in "${PDF_FILES[@]}"; do
    PDF_INDEX=$((PDF_INDEX + 1))
    PDF_NAME=$(basename "$pdf_path")

    echo "[$PDF_INDEX/${#PDF_FILES[@]}] Processing: $PDF_NAME"

    # Parse PDF text
    python3 -m src.extraction.pdf_parser \
      --pdf "$pdf_path" \
      --output "/tmp/pdf_text_$PDF_INDEX.json"

    # Check if parsing succeeded
    if [ $? -ne 0 ]; then
      echo "  âš ï¸  PDF parsing failed - skipping"
      continue
    fi

    # Check if text file exists and has content
    if [ ! -f "/tmp/pdf_text_$PDF_INDEX.json" ]; then
      echo "  âš ï¸  No text output - skipping"
      continue
    fi

    # Get text word count
    WORD_COUNT=$(jq -r '.text' "/tmp/pdf_text_$PDF_INDEX.json" 2>/dev/null | wc -w | tr -d ' ')
    echo "  ðŸ“„ Extracted $WORD_COUNT words"

    # Skip if too little text
    if [ "$WORD_COUNT" -lt 100 ]; then
      echo "  âš ï¸  Too little text ($WORD_COUNT words) - likely parsing error"
      continue
    fi

    # Get paper metadata
    DOI=$(echo "$PDF_NAME" | sed 's/.pdf$//' | sed 's/_/\//g')
    PAPER_METADATA=$(python3 -c "
import json
with open('$RUN_DIR/metadata/ranked_candidates.json') as f:
    papers = json.load(f)['papers']
    for p in papers:
        if '$DOI' in p.get('doi', ''):
            print(json.dumps(p))
            break
")

    echo "  ðŸ¤– Spawning quote_extractor agent..."

    # Notify UI
    python3 -c "from src.utils.ui_notifier import create_notifier; \
      create_notifier('$SESSION_ID').agent_spawn('quote_extractor', 'haiku')"

    # âš ï¸ CRITICAL: Use Task tool to spawn agent
    Task(
      subagent_type="general-purpose",
      model="haiku",
      description="Extract quotes from: $PDF_NAME",
      prompt="
You are a Quote Extractor Agent.

READ YOUR INSTRUCTIONS: .claude/agents/quote_extractor.md

Extract 2-3 relevant quotes for this research query: $QUERY

Paper metadata:
$PAPER_METADATA

PDF text (read from file):
$(cat /tmp/pdf_text_$PDF_INDEX.json | jq -r '.text')

IMPORTANT:
1. Run the pre-execution guard (validate PDF text)
2. Extract ONLY quotes that exist in the PDF text
3. Each quote must be â‰¤25 words
4. Include context_before and context_after
5. Return JSON format as specified in your instructions

Do NOT fabricate quotes! If you cannot find good quotes, return empty array.
"
    )

    # Notify agent complete
    python3 -c "from src.utils.ui_notifier import create_notifier; \
      create_notifier('$SESSION_ID').agent_complete('quote_extractor', 0)"

    # Collect quotes from agent output
    # Agent should return JSON - append to all_quotes.json
    echo "  âœ… Quotes extracted"
    QUOTE_COUNT=$((QUOTE_COUNT + 3))  # Approximate

    echo ""
  done

  echo "ðŸ“Š Quote extraction complete:"
  echo "  Total quotes: ~$QUOTE_COUNT"
  echo ""
fi
```

**Step 2: Consolidate Quotes**
```bash
# Merge all quote results into single file
# (This assumes agents wrote their outputs to temporary files)
# Linear coordinator should collect agent outputs and merge them

echo "ðŸ’¾ Saving quotes to: $RUN_DIR/metadata/quotes.json"
cp /tmp/all_quotes.json $RUN_DIR/metadata/quotes.json
```

**Step 4: Notify Phase Complete**
```bash
QUOTE_COUNT=$(jq '[.quotes] | add | length' /tmp/all_quotes.json 2>/dev/null || echo 0)
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(6, 'Quote Extraction')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(90)"
```

**Error Handling:**
- PDF parse fails â†’ Skip paper, log error
- Quote extractor fails â†’ Try keyword-based fallback
- No quotes found â†’ Mark paper but continue

---

### Phase 7: Export Results

**Goal:** Export all results to run directory with source annotation

**Step 0: Notify Phase Start**
```bash
source /tmp/run_config.env
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(7, 'Export')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').progress(95)"
```

**Steps:**

1. **Load Run Config:**
```bash
source /tmp/run_config.env
# $RUN_DIR, $CITATION_STYLE, $SESSION_ID now available
```

2. **Export JSON:**
```bash
# Compile final results
cat > $RUN_DIR/results.json <<EOF
{
  "session_id": "$(basename $RUN_DIR)",
  "query": "$QUERY",
  "mode": "$MODE",
  "citation_style": "$CITATION_STYLE",
  "statistics": {
    "papers_found": 97,
    "papers_from_api": 47,
    "papers_from_dbis": 50,
    "papers_ranked": 25,
    "pdfs_downloaded": 23,
    "quotes_extracted": 48,
    "sources": {
      "crossref": 20,
      "openalex": 15,
      "semantic_scholar": 12,
      "IEEE Xplore": 18,
      "ACM Digital Library": 16,
      "JSTOR": 16
    }
  },
  "papers": $(cat /tmp/ranked_papers.json),
  "quotes": $(cat /tmp/all_quotes.json)
}
EOF
```

3. **Export CSV with Citations:**
```bash
python3 -m src.export.csv_exporter \
  --quotes /tmp/all_quotes.json \
  --papers /tmp/ranked_papers.json \
  --style $CITATION_STYLE \
  --output $RUN_DIR/quotes.csv
```

4. **Export Markdown Summary:**
```bash
python3 -m src.export.markdown_exporter \
  --results $RUN_DIR/results.json \
  --output $RUN_DIR/summary.md
```

5. **Export BibTeX:**
```bash
python3 -m src.export.bibtex_exporter \
  --papers /tmp/ranked_papers.json \
  --output $RUN_DIR/bibliography.bib
```

6. **Copy Temp Files:**
```bash
# Archive intermediate files for debugging
cp /tmp/queries.json $RUN_DIR/temp/
cp /tmp/search_results.json $RUN_DIR/temp/
cp /tmp/ranked_papers.json $RUN_DIR/temp/
cp /tmp/all_quotes.json $RUN_DIR/temp/
```

7. **Stop Logging:**
```bash
python3 -m src.utils.logger --run-dir "$RUN_DIR" --message "Export complete" --level INFO
python3 -m src.utils.logger --run-dir "$RUN_DIR" --stop
```

8. **Delete Checkpoint:**
```bash
# Remove checkpoint after successful completion
rm -f $RUN_DIR/checkpoint.json
```

9. **Notify Phase Complete & Final Status:**
```bash
# Count final stats
PAPERS_TOTAL=$(jq '.papers | length' /tmp/ranked_papers.json)
PDFS_TOTAL=$(ls $RUN_DIR/pdfs/*.pdf 2>/dev/null | wc -l | tr -d ' ')
QUOTES_TOTAL=$(jq '[.quotes] | add | length' /tmp/all_quotes.json 2>/dev/null || echo 0)

# Notify completion
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(7, 'Export')"
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').complete($PAPERS_TOTAL, $PDFS_TOTAL, $QUOTES_TOTAL)"
```

10. **Show User:**
```
âœ“ Research Complete!

Found: 47 papers
Ranked: 25 papers
PDFs: 22/25 (88%)
Quotes: 45 relevant quotes

Results saved to: runs/2026-02-27_14-30-00/
â”œâ”€â”€ pdfs/ (22 PDFs, 88 MB)
â”œâ”€â”€ results.json (full results)
â”œâ”€â”€ quotes.csv (45 quotes with APA7 citations)
â”œâ”€â”€ summary.md (markdown report)
â”œâ”€â”€ bibliography.bib (BibTeX for LaTeX)
â”œâ”€â”€ session.db (SQLite database)
â”œâ”€â”€ session_log.txt (execution logs)
â””â”€â”€ temp/ (intermediate files)
```

**Error Handling:**
- Export fails â†’ Log error but don't fail whole session
- At least results.json should be saved
- Partial exports are OK (e.g., CSV fails but JSON succeeds)

---

## State Management

Throughout workflow, maintain state in SQLite:

```bash
# Session table
INSERT INTO sessions (id, query, mode, status, created_at)

# Papers table
INSERT INTO papers (session_id, doi, title, score, pdf_path)

# Quotes table
INSERT INTO quotes (paper_id, quote, page, relevance)
```

Access via Python:
```bash
python -m src.state.state_manager --action save --data "$JSON"
```

---

## Progress Feedback

**IMPORTANT:** Send live updates to Web UI + Console

### Web UI Updates (v2.3+)

Use Python UINotifier module to send live updates:

```bash
# Initialize notifier (auto-detects if Web UI is running)
python3 -c "
from src.utils.ui_notifier import create_notifier
import json

SESSION_ID = '$(basename $RUN_DIR)'
notifier = create_notifier(SESSION_ID)

# Phase start
notifier.phase_start(1, 'Context Setup')
notifier.progress(10)

# Papers found
notifier.papers_found(47)

# PDFs downloaded
notifier.pdfs_downloaded(22)

# Phase complete
notifier.phase_complete(1, 'Context Setup')
"
```

**Send updates after EVERY major action:**
- Phase start/complete
- Papers found (update counter)
- PDFs downloaded (update counter)
- Agent spawning/completion
- Errors

**Example - Phase 3 with UI Updates:**
```bash
# Phase start
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_start(3, 'API Search')"

# Search APIs
python -m src.search.search_engine --query "..." --output /tmp/results.json

# Update UI with results
PAPERS_FOUND=$(jq '.papers | length' /tmp/results.json)
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').papers_found($PAPERS_FOUND)"

# Phase complete
python3 -c "from src.utils.ui_notifier import create_notifier; create_notifier('$SESSION_ID').phase_complete(3, 'API Search')"
```

### Console Output

Also show user progress in console:

```
Phase 1/7: Context Setup âœ“
Phase 2/7: Query Generation âœ“
Phase 3/7: Searching APIs... (found 47 papers)
Phase 4/7: Ranking... (top 25 selected)
Phase 5/7: Downloading PDFs... (22/25 successful)
Phase 6/7: Extracting Quotes... (45 quotes found)
Phase 7/7: Exporting Results... âœ“

âœ“ Research Complete!
```

---

## Error Recovery

### Graceful Degradation:

1. **query_generator Agent fails** â†’ Use original user query
2. **llm_relevance_scorer Agent fails** â†’ Use 5D scores only
3. **PDF download fails** â†’ Continue with available PDFs
4. **quote_extractor Agent fails** â†’ Return papers without quotes
5. **DBIS browser timeout** â†’ Skip paper, continue

### Resume Capability:

If workflow interrupted:
```bash
# Checkpoint saved automatically every phase
# User can resume:
/research --resume session_123
```

---

## Testing

Test with these queries:

1. **Quick Test:** "DevOps" (should complete in <10 min)
2. **Standard Test:** "DevOps Governance" (should complete in ~35 min)
3. **Deep Test:** "Software Engineering Governance Frameworks" (should complete in ~60 min)

**Success Criteria:**
- âœ… All 6 phases complete
- âœ… 15+ papers found
- âœ… 85-90% PDF download rate (with DBIS)
- âœ… 2+ quotes per paper
- âœ… Results saved to JSON + Database

---

## Important Notes

1. **Subagent Spawning:**
   - Use Task tool (not Bash!)
   - Wait for completion
   - Handle timeouts gracefully

2. **Python Module Calls:**
   - Use Bash tool
   - Check exit codes
   - Parse JSON outputs

3. **DBIS Browser:**
   - Agent opens visible browser
   - User performs manual login
   - Agent waits up to 5 min
   - Continue with other papers if timeout

4. **No API Keys Needed:**
   - All LLM calls via Claude Code agents
   - No Anthropic API key required
   - Python modules use anonymous APIs

---

**Coordinator End**
