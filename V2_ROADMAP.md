# Academic Agent v2.0 - Roadmap

**Erstellt:** 2026-02-23
**Status:** Architektur finalisiert - Szenario B (Smart-LLM) gewÃ¤hlt
**Ziel:** Ein zuverlÃ¤ssiges KI-Agenten-System fÃ¼r akademische Recherche
**Erfolgsmetrik:** 85-92% Erfolgsrate, vollstÃ¤ndig autonom, transparent

---

## ğŸ“Œ Executive Summary

### Kern-Architektur v2.0

```
1 Sonnet-Agent (Coordinator)
  â†“
  â”œâ”€ 3 Haiku-Agents (Semantik)
  â”‚   â”œâ”€ QueryGenerator
  â”‚   â”œâ”€ FiveDScorer-Relevanz (Hybrid)
  â”‚   â””â”€ QuoteExtractor
  â”‚
  â””â”€ 10 Python-Module (Deterministisch)
      â”œâ”€ API-Clients (CrossRef, OpenAlex, S2)
      â”œâ”€ PDF-Fetcher (Unpaywall, CORE, DBIS-Browser)
      â”œâ”€ StateManager, Deduplicator
      â””â”€ ProgressUI, QuoteValidator
```

### Key Metrics (v1.0 â†’ v2.0)

| Metrik | v1.0 | v2.0 (Szenario B) | Verbesserung |
|--------|------|-------------------|--------------|
| **Erfolgsrate** | 60% | 85-92% | +42% |
| **Manuelle Interventionen** | 4x pro Run | 0-1x | -75% |
| **Cost pro Run** | $2.15 | $0.27 | -87% |
| **Dauer Quick Mode** | 35 Min | 15-20 Min | -43% |
| **Relevanz-Ranking** | 70-75% | 92-95% | +25% |
| **PDF-Download** | 17% | 85-90% | +470% |

**Entwicklungszeit:** 14-16 Wochen
**VollstÃ¤ndige Docs:** [V2_ROADMAP_FULL.md](V2_ROADMAP_FULL.md), [MODULE_TYPES_OVERVIEW.md](MODULE_TYPES_OVERVIEW.md)

---

## ğŸ¯ Vision

**Von:** Fragiles Multi-Agent-System mit 60% Erfolgsrate
**Zu:** Robustes, API-first Hybrid-System mit 85-92% ZuverlÃ¤ssigkeit

### Kernprinzipien v2.0
1. **API-First**: VerlÃ¤ssliche APIs statt fragiles Web-Scraping
2. **Simplicity**: Linear statt komplex-hierarchisch
3. **Quality**: LLM wo nÃ¶tig (Szenario B), Python wo mÃ¶glich
4. **Transparency**: User sieht jeden Schritt in Echtzeit
5. **Resilience**: Graceful Degradation bei Fehlern
6. **Speed**: 15-20 Min statt 35+ Min fÃ¼r Quick Mode

---

## ğŸ“Š Problem-Analyse v1.0

### Kritische Fehler (Must Fix)

#### 1. Orchestrator-Agent versagt âŒ CRITICAL
**Problem:**
- Orchestrator spawnt keine Sub-Agents nach Phase 1
- Workflow bricht ab, benÃ¶tigt 4x manuelle Intervention
- Versprochen: Autonom | RealitÃ¤t: NICHT verwendbar

**Root Cause:**
- Zu komplexe Agent-Hierarchie (Orchestrator â†’ 5 Sub-Agents)
- Task-Tool Kommunikation funktioniert nicht zuverlÃ¤ssig
- Asynchrone Agent-Koordination fehlerhaft

**v2.0 LÃ¶sung:** Linear Coordinator (1 Agent) + Python-Module (KEIN Task-Tool Spawning!)

---

#### 2. Web-Scraping instabil âŒ HIGH
**Problem:**
- ACM/IEEE/Scopus Selektoren "veraltet" â†’ Nur Google Scholar
- 5/6 PDF-Downloads fehlgeschlagen (ResearchGate 403, ProQuest Auth)
- Jede UI-Ã„nderung bricht Selektoren

**v2.0 LÃ¶sung:** APIs (CrossRef, OpenAlex, Semantic Scholar) + DBIS-Browser fÃ¼r PDFs

---

#### 3. User Transparency fehlt âŒ HIGH
**Problem:**
- Headless Browser â†’ User sieht nichts
- Live-Monitor (tmux) funktioniert nicht
- User-Zitat: "wirkt so als wÃ¼rdest du nichts machen"

**v2.0 LÃ¶sung:** Headful Browser + stdout Progress Bars (rich library)

---

### Was funktioniert âœ… (Keep & Improve)

1. **Suchstring-Generierung** (10/10) - KI-gestÃ¼tzte Boolean-Queries â†’ Behalten + API-optimieren
2. **5D-Scoring-Methodik** (8/10) - Relevanz/Recency/Quality/Authority â†’ Behalten + Citations via API
3. **Zitat-Extraktion** (9/10) - 18 perfekte Zitate (â‰¤25 WÃ¶rter) â†’ Behalten + PDF-Validierung
4. **JSON State Management** (8/10) - research_state.json â†’ Behalten + SQLite fÃ¼r Queries

---

## ğŸ—ï¸ Architektur v2.0

### Architektur-Entscheidung: Szenario B (Smart-LLM)

**ENTSCHEIDUNG (2026-02-23):** v2.0 nutzt **Szenario B** - QualitÃ¤t vor Kosten!

| Kriterium | Szenario A (Minimal-LLM) | Szenario B (Smart-LLM) | Gewinner |
|-----------|--------------------------|------------------------|----------|
| Cost pro Run | $0.17 | $0.27 | A (gÃ¼nstiger) |
| Relevanz-Ranking | 80-85% gut | 92-95% gut | âœ… B |
| False-Positives | 15-20% | 5-8% | âœ… B |
| Semantik | âŒ Keyword-basiert | âœ… LLM-gestÃ¼tzt | âœ… B |
| User-Zufriedenheit | Mittel | Hoch | âœ… B |

**Bottom Line:** +$0.10 fÃ¼r 10-15% bessere QualitÃ¤t ist es wert!

---

### Warum Linear Coordinator statt Multi-Agent?

#### v1.0 Problem: Multi-Agent-Hierarchie
```
Orchestrator Agent
  â†“ (Task-Tool Spawning - fehleranfÃ¤llig!)
  â”œâ”€ Search Agent
  â”œâ”€ Browser Agent
  â”œâ”€ Scoring Agent
  â”œâ”€ Extract Agent
  â””â”€ Setup Agent
```

**Fehler:** Asynchrone Agent-Koordination, 40% Spawn-Fehler

#### v2.0 LÃ¶sung: Linear Coordinator + Module
```
Linear Coordinator (1 Sonnet Agent)
  â†“ (Direkte Python-Aufrufe - synchron!)
  â”œâ”€ SearchEngine.search()
  â”œâ”€ FiveDScorer.score()
  â”œâ”€ PDFFetcher.fetch()
  â””â”€ QuoteExtractor.extract()
```

**Vorteile:**
- âœ… Kein Task-Tool Spawning (auÃŸer Initial-Start)
- âœ… Synchrone AusfÃ¼hrung (deterministisch)
- âœ… Ein Agent, ein Stack Trace (debugbar)
- âœ… Module testbar (Unit + Integration Tests)

---

### Ordnerstruktur v2.0

```
.claude/
â”œâ”€â”€ agents/                      # 4 Agent-Prompts (.md)
â”‚   â”œâ”€â”€ linear_coordinator.md   # Sonnet - Haupt-Coordinator
â”‚   â”œâ”€â”€ query_generator.md      # Haiku - Boolean-Queries
â”‚   â”œâ”€â”€ five_d_scorer.md        # Haiku - Relevanz-Scoring
â”‚   â””â”€â”€ quote_extractor.md      # Haiku - Zitat-Extraktion
â”‚
â””â”€â”€ skills/research/skill.py    # User-Command: /research

src/                            # 10 Python-Module
â”œâ”€â”€ coordinator/                # Agent-Execution
â”œâ”€â”€ search/                     # API-Clients (CrossRef, OpenAlex, S2)
â”œâ”€â”€ ranking/                    # 5D-Scoring, Citations
â”œâ”€â”€ pdf/                        # PDFFetcher + DBIS-Browser â† KILLER-FEATURE!
â”œâ”€â”€ extraction/                 # Quote-Validation
â”œâ”€â”€ state/                      # SQLite + JSON
â”œâ”€â”€ ui/                         # Progress Bars (rich)
â””â”€â”€ utils/                      # Rate-Limiter, Retry, Cache

tests/
â”œâ”€â”€ unit/                       # 80%+ Coverage
â”œâ”€â”€ integration/                # API-Clients, PDF-Chain
â””â”€â”€ e2e/                        # Full Workflow Tests

docs/
â”œâ”€â”€ ARCHITECTURE_v2.md          # Detaillierte Architektur
â”œâ”€â”€ MODULE_SPECS_v2.md          # Modul-Spezifikationen + Code
â”œâ”€â”€ PROBLEM_ANALYSIS_v1.md      # v1.0 Post-Mortem
â””â”€â”€ PDF_ACQUISITION_FLOW.md     # DBIS-Browser Flow-Chart
```

**Detaillierte Infos:** Siehe [V2_ROADMAP_FULL.md](V2_ROADMAP_FULL.md) fÃ¼r Code-Beispiele

---

## ğŸ“… Phasen & Timeline (14-16 Wochen)

### Phase 0: Foundation (Woche 1-2)
**Ziel:** Neue Basis-Infrastruktur ohne alte KomplexitÃ¤t

**Meilensteine:**
- API-Accounts (CrossRef, OpenAlex, S2, Unpaywall, CORE)
- Agent-Definitionen erstellen (4x .md Prompts)
- API-Client-Library (rate-limiting, retry, caching)
- SQLite Schema (Candidates, Papers, Quotes)
- Linear Workflow Skeleton (6 sequentielle Steps)
- Haiku-Integration testen (QueryGenerator Prototype)

**Akzeptanzkriterien:**
- API-Calls funktionieren mit Rate-Limiting
- SQLite speichert & liest korrekt
- Workflow fÃ¼hrt 6 Dummy-Steps aus
- Haiku-Call funktioniert

---

### Phase 1: Search Engine (Woche 3-4)
**Ziel:** API-basierte Paper-Suche, 95%+ Erfolgsrate

**Meilensteine:**
- CrossRef, OpenAlex, Semantic Scholar API Integration
- Query-Generator v2 (API-optimiert, Haiku-gestÃ¼tzt)
- Multi-Source-Deduplication (DOI-basiert)
- Fallback auf Google Scholar (wenn APIs <10 Results)

**Akzeptanzkriterien:**
- 15+ Papers in <2 Min (statt 7 Min in v1)
- 90%+ Peer-Reviewed (statt 57% in v1)
- 100% DOI Coverage (statt 30% in v1)

---

### Phase 2: Ranking Engine (Woche 5)
**Ziel:** 5D-Scoring v2 mit LLM-Relevanz (Szenario B)

**Meilensteine:**
- 5D-Scoring aus v1 migrieren
- **LLM-Relevanz-Scoring** (Haiku) - Semantisches VerstÃ¤ndnis
- Citation-Count Integration (OpenAlex)
- Journal Impact Factor (OpenAlex venue data)

**Akzeptanzkriterien:**
- Relevanz-Ranking: 92-95% PrÃ¤zision
- Top 3 Papers haben >80% Relevanz-Score

---

### Phase 3: PDF Acquisition (Woche 6-8) ğŸ”¥ KILLER-FEATURE
**Ziel:** 85-90% PDF-Download-Erfolgsrate (statt 17% in v1)

**Strategie: 3-Step Fallback-Chain**
```
1. Unpaywall API    â†’ 40% Erfolg (1-2s)
2. CORE API         â†’ +10% Erfolg (2s)
3. DBIS Browser     â†’ +35-40% Erfolg (15-25s, INSTITUTIONAL ACCESS!)
```

**Meilensteine:**
- **Woche 6:** Unpaywall + CORE API Clients
- **Woche 7:** DBIS Browser (Playwright, Shibboleth-Auth, Publisher-Navigation)
- **Woche 8:** Rate-Limiting (10-20s), Fallback-Chain, Testing

**DBIS-Browser Details:**
- Headful Browser (User sieht alles!)
- TIB Shibboleth-Authentifizierung
- Publisher-Navigation (IEEE, ACM, Springer, Elsevier)
- Human-Like Behavior (10-20s Delays, Maus-Bewegungen)

**Akzeptanzkriterien:**
- 85-90% PDFs erfolgreich downloaded
- 10-15% Papers Ã¼bersprungen (kein Manual-Wait!)
- Headful Browser sichtbar
- Keine Account-Sperrung in Tests

---

### Phase 4: Quote Extraction (Woche 9)
**Ziel:** v1 System migrieren + Validierung

**Meilensteine:**
- v1 Extraction-Logik nach v2 portieren (Haiku)
- PDF-Text-Validierung (Quote wirklich im PDF?)
- Context-Window erweitern (50 WÃ¶rter vor/nach)

**Akzeptanzkriterien:**
- 100% Zitate validiert gegen PDF-Text
- â‰¤25 WÃ¶rter Compliance: 100%

---

### Phase 5: User Experience (Woche 10)
**Ziel:** Transparenz, Echtzeit-Feedback

**Meilensteine:**
- Real-time stdout Progress Bar (rich library)
- Live Metrics Dashboard (CLI)
- User-friendly Error Messages

**UI-Beispiel:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Phase 2/6: Searching APIs                    â•‘
â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  50% (7/15 Papers)         â•‘
â•‘  âœ… CrossRef: 5 papers (3s)                   â•‘
â•‘  â³ OpenAlex: In Progress...                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### Phase 6: Testing & Reliability (Woche 11-12)
**Ziel:** 85-92% Erfolgsrate durch Tests

**Meilensteine:**
- Unit Tests (80%+ Coverage)
- Integration Tests (alle API-Clients)
- E2E Tests (5 verschiedene Themen)
- Stress Tests (Rate-Limiting, API-AusfÃ¤lle)

**Test-Szenarien:**
1. Happy Path: 15 Papers, alle PDFs verfÃ¼gbar
2. Partial Fail: 15 Papers, 5 PDFs fehlgeschlagen
3. API Outage: CrossRef down â†’ Fallback OpenAlex
4. Rate Limit: 100 Requests/min exceeded

---

### Phase 7: Migration & Cleanup (Woche 13-14)
**Ziel:** v1 â†’ v2 Migration

**Meilensteine:**
- v1 Code archivieren (in legacy/)
- v2 als Default-System setzen
- Documentation Update
- Performance Benchmarks dokumentieren

**Was lÃ¶schen:**
- .claude/agents/orchestrator-agent.md (broken)
- 50+ obsolete Shell-Scripts

**Was behalten:**
- 5D-Scoring Logik
- Quote-Extraction Logik
- JSON Schemas

---

## ğŸ¯ Success Criteria & Go/No-Go

### MUSS erfÃ¼llt sein (alle!):
- âœ… Agent-Prompt â‰¤500 Zeilen, â‰¤120 Zeichen/Zeile
- âœ… Erfolgsrate â‰¥85%
- âœ… 0 manuelle Interventionen in 10 Test-LÃ¤ufen
- âœ… PDF-Download â‰¥85%
- âœ… Unit Test Coverage â‰¥70%

### SOLLTE erfÃ¼llt sein (3 von 5):
- âš ï¸ Erfolgsrate â‰¥90%
- âš ï¸ Dauer â‰¤20 Min
- âš ï¸ PDF-Download â‰¥90%
- âš ï¸ Peer-Review â‰¥95%
- âš ï¸ Unit Test Coverage â‰¥80%

### NO-GO wenn:
- ğŸ”´ Agent-Prompt >600 Zeilen (zu komplex!)
- ğŸ”´ Erfolgsrate <80%
- ğŸ”´ >1 manuelle Intervention pro Lauf

---

## ğŸ“Š KPI Tracking

### Erfolgsrate-Messung

```python
# E2E-Test mit 20 verschiedenen Queries
def measure_success_rate():
    success_count = 0
    for query in test_queries:
        result = coordinator.run(query)
        if result.success and result.quotes_count >= 10:
            success_count += 1
    return success_count / len(test_queries) * 100
```

**Ziel:** â‰¥85% (17/20 erfolgreiche LÃ¤ufe)

---

## ğŸ“š WeiterfÃ¼hrende Dokumentation

- **[V2_ROADMAP_FULL.md](V2_ROADMAP_FULL.md)** - VollstÃ¤ndige Roadmap mit Code-Beispielen (109KB)
- **[MODULE_TYPES_OVERVIEW.md](MODULE_TYPES_OVERVIEW.md)** - Modul-Ãœbersicht mit LLM-Entscheidungen
- **[ARCHITECTURE_v2.md](ARCHITECTURE_v2.md)** - Detaillierte Architektur-Dokumentation (in Arbeit)
- **[MODULE_SPECS_v2.md](MODULE_SPECS_v2.md)** - Modul-Spezifikationen + Code (in Arbeit)
- **[PROBLEM_ANALYSIS_v1.md](PROBLEM_ANALYSIS_v1.md)** - v1.0 Post-Mortem (in Arbeit)

---

## ğŸš€ Next Steps

1. **Woche 1-2:** Phase 0 - Foundation starten
2. **API-Accounts erstellen:** CrossRef, OpenAlex, Semantic Scholar, Unpaywall, CORE
3. **Agent-Definitionen schreiben:** 4x .md Prompts (linear_coordinator, query_generator, five_d_scorer, quote_extractor)
4. **SQLite Schema designen:** Candidates, Papers, Quotes Tabellen

**Los geht's! ğŸ¯**
