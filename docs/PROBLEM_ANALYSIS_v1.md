# Academic Agent v1.0 - Problem-Analyse & Post-Mortem

**Erstellt:** 2026-02-23
**Ziel:** Dokumentation der v1.0 Fehler als Lerngrundlage f√ºr v2.0

---

## üìä v1.0 Performance-√úbersicht

### Erfolgsrate: 60% (6.3/10)

| Metrik | Wert | Status |
|--------|------|--------|
| Erfolgsrate | 60% | ‚ùå Zu niedrig |
| Manuelle Interventionen | 4x pro Run | ‚ùå Nicht autonom |
| PDF-Download | 17% | ‚ùå Kritisch niedrig |
| Peer-Review Coverage | 57% | ‚ö†Ô∏è Unzureichend |
| Dauer Quick Mode | 35 Min | ‚ö†Ô∏è Zu langsam |
| Cost pro Run | $2.15 | ‚ö†Ô∏è Zu teuer |

---

## ‚ùå Kritische Fehler (Must Fix)

### 1. Orchestrator-Agent versagt (CRITICAL)

**Problem:**
- Orchestrator spawnt keine Sub-Agents nach Phase 1
- Workflow bricht ab, ben√∂tigt manuelle Intervention
- Versprochen: Autonom | Realit√§t: 4x manuelle Agent-Starts pro Run

**Symptome:**
```
Phase 1: Setup ‚úÖ
Phase 2: Search ... ‚è≥ (startet nicht)
‚Üí User muss manuell Search-Agent spawnen
Phase 3: Browser ... ‚è≥ (startet nicht)
‚Üí User muss manuell Browser-Agent spawnen
```

**Root Cause:**
- Zu komplexe Agent-Hierarchie (Orchestrator ‚Üí 5 Sub-Agents)
- Task-Tool Kommunikation funktioniert nicht zuverl√§ssig
- Asynchrone Agent-Koordination fehlerhaft
- Agent-Spawning hat 40% Fehlerrate

**Impact:**
- System ist NICHT autonom verwendbar
- User-Frustration extrem hoch
- Produktiv-Einsatz unm√∂glich

**v2.0 L√∂sung:**
- Linear Coordinator (1 Agent) + Python-Module
- KEIN Task-Tool Spawning (au√üer Initial-Start)
- Synchrone Ausf√ºhrung (deterministisch)

---

### 2. Web-Scraping instabil (HIGH)

**Problem:**
- ACM/IEEE/Scopus Selektoren "veraltet" ‚Üí Nur Google Scholar funktioniert
- 5/6 PDF-Downloads fehlgeschlagen
- ResearchGate: 403 Forbidden
- ProQuest: Auth-Problem
- Jede UI-√Ñnderung bricht Selektoren

**Root Cause:**
- CSS-Selektoren √§ndern sich st√§ndig
- Anti-Bot-Protection (403 Forbidden)
- Institutional Access nicht implementiert
- Headless Browser wird erkannt

**Impact:**
- Niedrige Paper-Qualit√§t
- Manuelle PDF-Downloads n√∂tig
- Nur 17% PDF-Erfolgsrate

**v2.0 L√∂sung:**
- APIs (CrossRef, OpenAlex, Semantic Scholar) statt Scraping
- DBIS-Browser mit Institutional Access (TIB)
- Headful Browser (transparenter f√ºr User)
- Fallback-Chain: Unpaywall ‚Üí CORE ‚Üí DBIS

---

### 3. User Transparency fehlt (HIGH)

**Problem:**
- Headless Browser ‚Üí User sieht nichts
- Live-Monitor (tmux) funktioniert nicht
- User-Zitat: "wirkt so als w√ºrdest du nichts machen"
- Status-Updates zu selten

**Root Cause:**
- Falsches UX-Design (headless statt headful)
- Monitoring zu komplex (tmux statt stdout)
- Keine Real-time Progress Bars

**Impact:**
- User verliert Vertrauen
- User f√ºhlt sich hilflos
- Unklare Fehler-Kommunikation

**v2.0 L√∂sung:**
- Headful Browser (User sieht Browser-Navigation)
- stdout Progress Bars (rich library)
- Live Metrics Dashboard (CLI)
- User-friendly Error Messages

---

## ‚ö†Ô∏è Weitere Probleme

### 4. PDF-Download fehlerhaft (CRITICAL)

**Problem:**
- 5/6 PDFs fehlgeschlagen (83% Fehlerrate!)
- Nur 17% Erfolgsrate bei 6 Test-Papers
- Kein Institutional Access

**Root Cause:**
- Direct Download ohne Institutional Access
- Keine API-First-Strategie
- Kein Fallback-Mechanismus

**v2.0 L√∂sung:**
- 3-Step Fallback: Unpaywall ‚Üí CORE ‚Üí DBIS
- DBIS mit TIB Shibboleth-Auth
- Ziel: 85-90% Erfolgsrate

---

### 5. Niedrige Peer-Review Coverage (MEDIUM)

**Problem:**
- Nur 57% Peer-reviewed Papers
- Rest: Conference Talks, Blog Posts, Preprints

**Root Cause:**
- Google Scholar filtert nicht nach Peer-Review
- Keine Qualit√§tskontrolle

**v2.0 L√∂sung:**
- CrossRef API (nur Peer-reviewed Journals)
- OpenAlex Venue-Filter
- Ziel: 95%+ Peer-reviewed

---

### 6. Performance zu langsam (MEDIUM)

**Problem:**
- Quick Mode: 35 Minuten (Ziel: 15-20 Min)
- Paper-Suche: 7 Minuten (zu lange!)

**Root Cause:**
- Web-Scraping langsamer als API-Calls
- Kein Parallelismus
- Zu viele Timeouts

**v2.0 L√∂sung:**
- API-Calls statt Scraping (1-2 Min statt 7 Min)
- Parallele API-Requests
- Ziel: 15-20 Min Quick Mode

---

### 7. Kosten zu hoch (LOW)

**Problem:**
- $2.15 pro Run (haupts√§chlich Sonnet-Calls)
- Orchestrator + 5 Sub-Agents = viele LLM-Calls

**Root Cause:**
- Zu viele Agent-Ebenen
- Overhead durch Agent-Koordination

**v2.0 L√∂sung:**
- 1 Sonnet + 3 Haiku + 10 Python-Module
- Python statt LLM wo m√∂glich
- Ziel: $0.27 pro Run (87% g√ºnstiger)

---

## ‚úÖ Was funktioniert (Keep & Improve)

### 1. Suchstring-Generierung (10/10)

**Was gut funktioniert:**
- KI-gest√ºtzte Boolean-Query-Erstellung
- Datenbank-spezifische Syntax
- Keyword-Clustering intelligent

**V2 Plan:**
- Behalten + API-optimierte Queries
- Haiku statt Sonnet (g√ºnstiger)

---

### 2. 5D-Scoring-Methodik (8/10)

**Was gut funktioniert:**
- Relevanz, Recency, Quality, Authority, Portfolio-Balance
- Duplikaterkennung funktioniert
- Transparente Gewichtung

**V2 Plan:**
- Behalten + Citation-Counts via API
- LLM-Relevanz-Scoring (Szenario B)

---

### 3. Zitat-Extraktion (9/10)

**Was gut funktioniert:**
- 18 perfekte Zitate extrahiert (‚â§25 W√∂rter)
- Kontext + Seitenzahlen + APA 7
- Thematische Clustering

**V2 Plan:**
- Behalten + Validierung gegen PDF
- Haiku statt Sonnet

---

### 4. JSON State Management (8/10)

**Was gut funktioniert:**
- research_state.json als Single Source of Truth
- 23 State-Updates erfolgreich
- Checkpointing funktioniert

**V2 Plan:**
- Behalten + SQLite f√ºr Querying
- JSON als Backup

---

## üìã Lessons Learned

### Do

- ‚úÖ API-First: Verl√§ssliche APIs statt fragiles Scraping
- ‚úÖ Simplicity: Linear statt komplex-hierarchisch
- ‚úÖ Transparency: User muss alles sehen k√∂nnen
- ‚úÖ Modularity: Python-Module statt Agent-Hierarchie
- ‚úÖ Testing: Unit + Integration + E2E Tests

### Don't

- ‚ùå Multi-Agent-Hierarchien (zu fehleranf√§llig)
- ‚ùå Asynchrone Agent-Koordination (Task-Tool)
- ‚ùå Web-Scraping als Prim√§rstrategie
- ‚ùå Headless Browser ohne User-Feedback
- ‚ùå Keine Test-Coverage

---

## üöÄ v2.0 Migration Path

1. **Phase 0:** Neue Architektur aufsetzen (Linear Coordinator)
2. **Phase 1-3:** Neue Module implementieren (Search, Ranking, PDF)
3. **Phase 4:** Funktionierende v1-Module migrieren (Quotes, Scoring)
4. **Phase 5-6:** UX + Testing
5. **Phase 7:** v1 archivieren, v2 als Default

**Was l√∂schen:**
- .claude/agents/orchestrator-agent.md
- 50+ obsolete Shell-Scripts
- Alte CDP-Wrapper

**Was behalten:**
- 5D-Scoring Logik
- Quote-Extraction Logik
- JSON Schemas

---

## üìö Verwandte Dokumentation

- [V2_ROADMAP.md](../V2_ROADMAP.md) - Neue Architektur & Timeline
- [ARCHITECTURE_v2.md](ARCHITECTURE_v2.md) - Architektur-Details
- [MODULE_SPECS_v2.md](MODULE_SPECS_v2.md) - Modul-Spezifikationen
