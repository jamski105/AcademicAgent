# Academic Agent v2.2 - Architektur-Dokumentation

**Erstellt:** 2026-02-23
**Aktualisiert:** 2026-02-27 (v2.2 - DBIS Search Integration)
**Ziel:** Agent-basierte Architektur Ã¼ber Claude Code + DBIS Meta-Portal

---

## ğŸ“‹ Ãœbersicht

### Architektur-Paradigma: Agent Orchestration via Claude Code

```
User â†’ Claude Code â†’ linear_coordinator Agent
  â†’ Spawnt Subagenten (query_gen, scorer, extractor, dbis_browser)
  â†’ Ruft Python-Module auf (search, ranking, parsing)
  â†’ Nutzt Chrome MCP fÃ¼r Browser Automation
```

**Kernprinzip:** Keine direkten Anthropic API-Calls, alles Ã¼ber Claude Code Agenten.

---

## ğŸ—ï¸ Neue Architektur v2.0

### High-Level Ãœbersicht

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: /research "DevOps Governance"                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SKILL.md (.claude/skills/research/)                     â”‚
â”‚ â†’ Spawnt: Task(subagent_type="linear_coordinator")     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LINEAR COORDINATOR AGENT (Sonnet 4.5)                   â”‚
â”‚ (.claude/agents/linear_coordinator.md)                  â”‚
â”‚                                                         â”‚
â”‚ Orchestriert 8 Phasen:                                 â”‚
â”‚                                                         â”‚
â”‚ Phase 1: Context Setup                                 â”‚
â”‚   â†’ Read config files (Bash: cat)                       â”‚
â”‚   â†’ Init database (Bash: python -m state.database)     â”‚
â”‚                                                         â”‚
â”‚ Phase 2: Query Generation                              â”‚
â”‚   â†’ Task(subagent="query_generator") â—„â”€â”€â”€ Haiku Agent  â”‚
â”‚                                                         â”‚
â”‚ Phase 2a: Discipline Classification (NEW v2.2)         â”‚
â”‚   â†’ Task(subagent="discipline_classifier") â—„â”€â”€â”€ Haiku  â”‚
â”‚   â†’ Maps query to DBIS categories                      â”‚
â”‚   â†’ Identifies relevant databases                      â”‚
â”‚                                                         â”‚
â”‚ Phase 3: Hybrid Search (ENHANCED v2.2)                 â”‚
â”‚   Track 1 (Fast):                                      â”‚
â”‚   â†’ Bash: python -m src.search.search_engine (APIs)    â”‚
â”‚   Track 2 (Comprehensive):                             â”‚
â”‚   â†’ Task(subagent="dbis_search") â—„â”€â”€â”€ Chrome MCP       â”‚
â”‚   â†’ Merges & deduplicates results                      â”‚
â”‚                                                         â”‚
â”‚ Phase 4: Ranking                                       â”‚
â”‚   â†’ Bash: python -m src.ranking.five_d_scorer          â”‚
â”‚   â†’ Task(subagent="llm_relevance_scorer") â—„â”€â”€â”€ Haiku   â”‚
â”‚                                                         â”‚
â”‚ Phase 5: PDF Acquisition                               â”‚
â”‚   â†’ Bash: python unpaywall + core clients              â”‚
â”‚   â†’ Task(subagent="dbis_browser") â—„â”€â”€â”€ Chrome MCP      â”‚
â”‚                                                         â”‚
â”‚ Phase 6: Quote Extraction                              â”‚
â”‚   â†’ Bash: python -m src.extraction.pdf_parser          â”‚
â”‚   â†’ Task(subagent="quote_extractor") â—„â”€â”€â”€ Haiku Agent  â”‚
â”‚                                                         â”‚
â”‚ Phase 7: Export Results (NEW v2.1)                     â”‚
â”‚   â†’ Bash: python -m src.export.csv_exporter            â”‚
â”‚   â†’ Bash: python -m src.export.markdown_exporter       â”‚
â”‚   â†’ Bash: python -m src.export.bibtex_exporter         â”‚
â”‚   â†’ Save to runs/{timestamp}/                          â”‚
â”‚                                                         â”‚
â”‚ Output: runs/2026-02-27_14-30-00/                      â”‚
â”‚   â”œâ”€â”€ pdfs/, results.json, quotes.csv                  â”‚
â”‚   â”œâ”€â”€ summary.md, bibliography.bib                     â”‚
â”‚   â””â”€â”€ session.db, logs                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Agents (Claude Code Subagents)

### 1. linear_coordinator (Sonnet 4.5)

**Rolle:** Master Orchestrator
**File:** `.claude/agents/linear_coordinator.md`
**Tools:** Bash, Read, Write, Task, Grep, Glob

**Verantwortlichkeiten:**
- Orchestriert 7-Phasen Workflow
- Spawnt Subagenten via Task tool
- Ruft Python-Module via Bash auf
- State Management (SQLite + Run Directory)
- Error Handling & Recovery
- Progress Tracking
- Export Management (CSV, Markdown, BibTeX)

### 2. query_generator (Haiku 4.5)

**Rolle:** Query Expansion
**File:** `.claude/agents/query_generator.md`
**Input:** User query, research mode, academic context
**Output:** Boolean queries, keywords, filters

**Verantwortlichkeiten:**
- Kreative Query-Expansion
- Synonyme & verwandte Begriffe
- Boolean-Query-Konstruktion
- API-spezifische Query-Optimierung

### 3. llm_relevance_scorer (Haiku 4.5)

**Rolle:** Semantische Relevanz-Bewertung
**File:** `.claude/agents/llm_relevance_scorer.md`
**Input:** Papers (Title, Abstract), User query
**Output:** Relevanz-Scores (0-1)

**Verantwortlichkeiten:**
- Semantisches VerstÃ¤ndnis von Paper-Inhalten
- Relevanz-Bewertung pro Paper
- Batch-Processing (10 papers)
- JSON Output fÃ¼r five_d_scorer

### 4. quote_extractor (Haiku 4.5)

**Rolle:** Zitat-Extraktion aus PDFs
**File:** `.claude/agents/quote_extractor.md`
**Input:** PDF Text, User query
**Output:** Relevante Zitate mit Context

**Verantwortlichkeiten:**
- Findet relevante Textstellen
- Extrahiert prÃ¤gnante Zitate (â‰¤25 WÃ¶rter)
- Kontext-Window (50 WÃ¶rter)
- Validierung gegen PDF-Text

### 5. dbis_browser (Sonnet 4.5)

**Rolle:** Browser Automation fÃ¼r PDF-Download
**File:** `.claude/agents/dbis_browser.md`
**Tools:** Chrome MCP (mcp__chrome__*)

**Verantwortlichkeiten:**
- DOI â†’ Publisher Website Navigation
- Paywall Detection
- Shibboleth Auth Flow (TIB Hannover)
- **Interaktiver Login** (User sieht Browser, Login manuell)
- PDF Download Link Detection
- Publisher-spezifische Flows:
  - IEEE Xplore
  - ACM Digital Library
  - Springer
  - Elsevier/ScienceDirect

---

## ğŸ Python-Module (CLI-fÃ¤hig)

### Phase 3: Search

**search_engine.py** (CLI)
```bash
python -m src.search.search_engine \
  --query "DevOps Governance" \
  --mode standard \
  --output results.json
```

**Integriert:**
- `crossref_client.py` - CrossRef API (50 req/s, anonymous)
- `openalex_client.py` - OpenAlex API (100 req/day, anonymous)
- `semantic_scholar_client.py` - S2 API (100 req/5min, anonymous)
- `deduplicator.py` - DOI-basierte Deduplizierung

### Phase 4: Ranking

**five_d_scorer.py** (CLI)
```bash
python -m src.ranking.five_d_scorer \
  --papers papers.json \
  --weights relevance:0.4,recency:0.2,quality:0.2,authority:0.2 \
  --output scored.json
```

**Features:**
- Relevanz (wird von llm_relevance_scorer Agent ergÃ¤nzt)
- Recency (log-scaled, max 10 Jahre)
- Quality (Citation Count, log-scaled)
- Authority (Venue-Heuristic)
- Portfolio Balance (optional)

### Phase 5: PDF Acquisition

**pdf_fetcher.py** (Wrapper)
- `unpaywall_client.py` - Unpaywall API (~40% Erfolg)
- `core_client.py` - CORE API (~10% zusÃ¤tzlich)
- Falls fehlgeschlagen â†’ Coordinator spawnt dbis_browser Agent

### Phase 6: Quote Extraction

**pdf_parser.py** (CLI)
```bash
python -m src.extraction.pdf_parser \
  --pdf paper.pdf \
  --output text.json
```

**Features:**
- PyMuPDF Text-Extraktion
- Page-by-page
- Text Cleaning & Normalization

---

## ğŸŒ Chrome MCP Integration

### Setup (.claude/settings.json)

```json
{
  "mcpServers": {
    "chrome": {
      "command": "npx",
      "args": ["-y", "@eddym06/custom-chrome-mcp@latest"],
      "env": {
        "CHROME_PATH": "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
      }
    }
  }
}
```

### dbis_browser Agent - Chrome MCP Tools

**VerfÃ¼gbare Tools:**
- `mcp__chrome__navigate` - URL Navigation
- `mcp__chrome__click` - Element Click
- `mcp__chrome__type` - Text Input
- `mcp__chrome__screenshot` - Screenshot
- `mcp__chrome__wait` - Wait for Element
- `mcp__chrome__evaluate` - JS Execution

**Workflow:**
1. Navigate zu DOI-URL
2. Redirect zu Publisher Website
3. Detect Paywall/Login
4. Falls TIB Shibboleth:
   - Navigate zu Shibboleth Login
   - **PAUSE fÃ¼r manuellen Login** (User sieht Browser!)
   - Screenshot zur BestÃ¤tigung
5. Find PDF Download Link
6. Click & Download
7. Return PDF Path

---

## ğŸŒ DBIS Search Architecture (NEW v2.2)

### Konzept: DBIS als Meta-Portal

**Problem:** Hunderte von Fachdatenbanken, jeweils eigene API/Interface
**LÃ¶sung:** DBIS (Database Information System) als einheitlicher Zugang

**Vorteil:**
- Eine Integration â†’ Zugang zu 100+ Datenbanken
- Automatische TIB-Lizenz Aktivierung
- Fachgebiets-basierte Selektion

### Phase 2a: Discipline Classification

**discipline_classifier Agent (Haiku)**

Input:
```json
{
  "user_query": "Lateinische Metrik",
  "expanded_queries": ["Latin Meter", "Classical Prosody", ...]
}
```

Output:
```json
{
  "primary_discipline": "Klassische Philologie",
  "secondary_disciplines": ["Literaturwissenschaft", "Linguistik"],
  "dbis_categories": ["2.1", "2.2"],  // DBIS Fachgebiet-IDs
  "relevant_databases": [
    "L'AnnÃ©e philologique",
    "JSTOR Classics",
    "Perseus Digital Library"
  ]
}
```

### Phase 3: DBIS Search Agent

**dbis_search Agent (Sonnet + Chrome MCP)**

**Workflow:**
```
1. Navigate zu DBIS Portal
   â†’ https://dbis.ur.de/UBTIB

2. Select Discipline
   â†’ Klickt Fachgebiet (z.B. "Klassische Philologie")

3. Filter for Licensed Databases
   â†’ Nur grÃ¼ne Ampel (TIB-Lizenz vorhanden)

4. For each relevant database:
   a) Click "Zur Datenbank"
      â†’ Aktiviert TIB-Lizenz via DBIS Redirect!

   b) Wait for database website load

   c) Find search interface
      â†’ Database-specific strategies (config/dbis_disciplines.yaml)

   d) Execute search
      â†’ Enter query, apply filters

   e) Extract results
      â†’ Scrape HTML for papers (Title, Authors, Year, DOI)
      â†’ Or use Export function (BibTeX/RIS if available)

   f) Return to DBIS for next database

5. Merge all results
   â†’ Annotate source (database name)
   â†’ Return to coordinator
```

**Database-Specific Strategies:**

```yaml
# config/dbis_disciplines.yaml
databases:
  "L'AnnÃ©e philologique":
    search_selector: "#search-field"
    search_type: "advanced"
    export_format: "bibtex"

  "JSTOR":
    search_selector: "input[name='Query']"
    search_type: "basic"
    result_selector: ".card--result"

  "IEEE Xplore":
    search_selector: "#xploreSearchInput"
    filters: ["Conference", "Journal"]
```

### DBIS Auto-Discovery (NEW v2.3)

**Problem:** Manually defining all databases for each discipline doesn't scale
- Jura: only 2 DBs defined, but DBIS has 20+
- New databases added to DBIS â†’ not automatically available
- 100+ databases Ã— 15 disciplines = too much manual config

**Solution:** Automatic Database Discovery

**Architecture:**

```
discipline_classifier â†’ discipline + dbis_url
                              â†“
                    dbis_search Agent
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â†“                     â†“
         Discovery Mode          Config Mode
         (Try First)             (Fallback)
                  â†“                     â†“
    1. Navigate to DBIS      Use predefined
       discipline page       databases from
    2. Scrape database       config file
       list from HTML
    3. Filter:
       - Green/yellow only
       - Blacklist applied
    4. Prioritize:
       - Preferred DBs first
       - Quality score
    5. Select TOP 3-5
                  â†“                     â†“
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    Search Selected DBs
```

**Discovery Algorithm:**

```python
def discover_databases(discipline_url, config):
    # 1. Navigate to DBIS discipline page
    driver.get(discipline_url)

    # 2. Extract all database entries
    databases = []
    for entry in find_all(".datenbank"):
        name = entry.find(".db-name").text
        traffic_light = entry.find("img[src*='amp']").attr("src")
        link = entry.find("a:contains('Zur Datenbank')").attr("href")

        # 3. Filter by traffic light (green/yellow only)
        if "amp_gruen" in traffic_light or "amp_gelb" in traffic_light:
            # 4. Apply blacklist
            if not any(blocked in name for blocked in BLACKLIST):
                databases.append({
                    "name": name,
                    "link": link,
                    "access": "free" if "gruen" in traffic_light else "tib"
                })

    # 5. Prioritize by preferred databases
    preferred = config.get("preferred_databases", [])
    databases.sort(key=lambda db: (
        0 if db["name"] in preferred else 1,  # Preferred first
        0 if db["access"] == "free" else 1,   # Green before yellow
        db["name"]                             # Alphabetical
    ))

    # 6. Select TOP N
    return databases[:config.get("discovery_max_databases", 5)]
```

**Blacklist (global):**
```yaml
discovery_blacklist:
  - "Katalog"         # Library catalogs
  - "Directory"       # Directories
  - "Encyclopedia"    # Reference works
  - "Handbook"        # Handbooks
  - "Lexikon"         # Lexica
```

**Config Example:**

```yaml
"Rechtswissenschaft":
  dbis_category_id: "9.1"
  dbis_url: "https://dbis.ur.de/dbis/dbliste.php?bib_id=ubtib&lett=f&sGeb=9.1"

  # Discovery Settings
  discovery_enabled: true          # Try discovery first
  discovery_max_databases: 5       # Select TOP 5

  # Preferred (if found during discovery, prioritize)
  preferred_databases:
    - "Beck-Online"
    - "Juris"
    - "HeinOnline"

  # Fallback (if discovery fails)
  fallback_databases:
    - name: "Beck-Online"
      priority: 1
    - name: "Juris"
      priority: 2
```

**Caching Strategy:**

```python
# Cache discovered databases for 24h
cache_key = f"dbis_discovery_{discipline}_{date.today()}"

if cache_key in cache:
    databases = cache.get(cache_key)
else:
    databases = discover_databases(discipline_url, config)
    cache.set(cache_key, databases, ttl=86400)  # 24h
```

**Why cache?**
- Discovery scraping is slow (~10-20 seconds)
- DBIS database list doesn't change daily
- Multiple queries same day â†’ reuse discovery

**Fallback Chain:**

```
1. Try Discovery
   â†“ (failed?)
2. Try fallback_databases from config
   â†“ (empty?)
3. Use general_databases (CrossRef, OpenAlex)
   â†“ (failed?)
4. Return empty + log error
```

**Performance Impact:**
- **First run (no cache):** +15 seconds (discovery scraping)
- **Subsequent runs (cached):** +0 seconds (instant)
- **Config mode:** +0 seconds (no discovery)

**Benefits:**
- ğŸ“ˆ **Scalability:** New DBIS databases automatically available
- ğŸ”„ **Maintainability:** Less manual config needed
- ğŸŒ **Coverage:** All disciplines get 100% DBIS coverage

---

### Result Merging

**Coordinator merges:**
- API Papers (CrossRef, OpenAlex, S2)
- DBIS Papers (all databases)

**Deduplication:**
- Primary: DOI matching
- Secondary: Title similarity (>85%)
- Keeps source annotation

**Source-Aware Ranking:**
- DBIS papers get +0.05 boost if discipline matches
- Reason: More likely to be relevant if found in specialized DB

---

## ğŸ’¾ State Management

### SQLite Database (state/database.py)

**Tables:**
- `sessions` - Research Sessions
- `papers` - Candidate Papers
- `quotes` - Extracted Quotes
- `checkpoints` - Resume Points

**Features:**
- Atomic Transactions
- Auto-Commit
- Checkpoint & Resume
- JSON Export

---

## ğŸ“ Repository-Struktur

```
AcademicAgent/
â”œâ”€â”€ setup.sh                       # â† Installation (inkl Chrome MCP)
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ settings.json             # â† Chrome MCP Config
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ linear_coordinator.md       # Master Agent
â”‚   â”‚   â”œâ”€â”€ query_generator.md          # Query Expansion
â”‚   â”‚   â”œâ”€â”€ discipline_classifier.md    # Discipline Detection (NEW v2.2)
â”‚   â”‚   â”œâ”€â”€ llm_relevance_scorer.md     # Relevanz-Bewertung
â”‚   â”‚   â”œâ”€â”€ quote_extractor.md          # Zitat-Extraktion
â”‚   â”‚   â”œâ”€â”€ dbis_browser.md             # PDF Download (Chrome MCP)
â”‚   â”‚   â””â”€â”€ dbis_search.md              # DBIS Search (NEW v2.2, Chrome MCP)
â”‚   â””â”€â”€ skills/research/
â”‚       â””â”€â”€ SKILL.md                 # Entry Point
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ research_modes.yaml          # Quick/Standard/Deep
â”‚   â”œâ”€â”€ dbis_disciplines.yaml        # DBIS Database Registry (NEW v2.2)
â”‚   â””â”€â”€ academic_context.md          # Optional User Context
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classification/                  # NEW v2.2
â”‚   â”‚   â””â”€â”€ discipline_classifier.py # CLI Module
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”œâ”€â”€ search_engine.py         # CLI Wrapper (Hybrid in v2.2)
â”‚   â”‚   â”œâ”€â”€ dbis_search_orchestrator.py  # NEW v2.2
â”‚   â”‚   â”œâ”€â”€ crossref_client.py
â”‚   â”‚   â”œâ”€â”€ openalex_client.py
â”‚   â”‚   â”œâ”€â”€ semantic_scholar_client.py
â”‚   â”‚   â””â”€â”€ deduplicator.py
â”‚   â”œâ”€â”€ ranking/
â”‚   â”‚   â””â”€â”€ five_d_scorer.py         # CLI Wrapper
â”‚   â”œâ”€â”€ pdf/
â”‚   â”‚   â”œâ”€â”€ pdf_fetcher.py           # Wrapper (Unpaywall+CORE)
â”‚   â”‚   â”œâ”€â”€ unpaywall_client.py
â”‚   â”‚   â””â”€â”€ core_client.py
â”‚   â”œâ”€â”€ extraction/
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py            # CLI Wrapper
â”‚   â”‚   â””â”€â”€ quote_validator.py
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â”œâ”€â”€ database.py              # SQLite Schema
â”‚   â”‚   â”œâ”€â”€ state_manager.py
â”‚   â”‚   â””â”€â”€ checkpointer.py
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ progress_ui.py
â”‚   â”‚   â””â”€â”€ error_formatter.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ rate_limiter.py
â”‚       â”œâ”€â”€ retry.py
â”‚       â””â”€â”€ cache.py
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â”œâ”€â”€ integration/
    â””â”€â”€ agents/                      # Agent Tests
```

---

## ğŸ”„ Workflow: User â†’ Result

```
1. User: /research "DevOps Governance"
   â†“
2. SKILL.md:
   - Mode Selection (Quick/Standard/Deep)
   - Load Configs
   - Spawn linear_coordinator Agent
   â†“
3. linear_coordinator Agent:
   Phase 1: Context Setup
   Phase 2: Query Gen â†’ query_generator Agent
   Phase 3: Search â†’ search_engine.py (Bash)
   Phase 4: Ranking â†’ five_d_scorer.py + llm_relevance_scorer Agent
   Phase 5: PDF â†’ unpaywall/core + dbis_browser Agent (fallback)
   Phase 6: Quotes â†’ pdf_parser.py + quote_extractor Agent
   â†“
4. Output: Research Results mit Zitaten
```

---

## ğŸ¯ Design-Prinzipien

1. **Agent-First:** Alle LLM-Calls via Claude Code Agenten
2. **No API Keys:** Keine direkten Anthropic API-Calls
3. **Chrome MCP:** Browser Automation via MCP (nicht Playwright)
4. **CLI-Python:** Module sind CLI-fÃ¤hig, von Agents aufrufbar
5. **Interaktiv:** User sieht Browser bei DBIS Login
6. **State-First:** Alles in SQLite, Resume-fÃ¤hig

---

FÃ¼r Details siehe:
- [MODULE_SPECS_v2.md](./MODULE_SPECS_v2.md) - Modul-Spezifikationen
- [WORKFLOW.md](../WORKFLOW.md) - Detaillierter Workflow
- [INSTALLATION.md](../INSTALLATION.md) - Setup-Anleitung
